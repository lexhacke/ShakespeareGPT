{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En16mH6OGWh-",
        "outputId": "4a918515-e06a-4fc9-dae0-db760e76b864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx6HLjXyGdcP",
        "outputId": "eb1fd08f-c5bf-4bc1-c170-be49c4cd9690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import heapq\n",
        "import random\n",
        "from tensorflow.keras  import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, LayerNormalization, Dense, Add, Concatenate, Dropout\n",
        "from keras.saving import register_keras_serializable\n",
        "from keras import config\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "token2vec = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BZC-phflEqvQ"
      },
      "outputs": [],
      "source": [
        "#global var\n",
        "context_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4a_nfw-BaGJN"
      },
      "outputs": [],
      "source": [
        "class embedify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  # The build() function is used to create weights that depend on the input shape, we M's dimensions set in constructor so it's all good\n",
        "  # The get_config() function is used to return a dict of what input params are needed to load this model from a save\n",
        "  def __init__(self, emb_dim, vocab_size, context_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_size = context_size\n",
        "    self.M = self.add_weight(shape=(vocab_size, emb_dim), initializer='glorot_uniform', name='M', trainable=True)\n",
        "    position = np.arange(context_size)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, emb_dim, 2) * -(np.log(10000.0) / emb_dim))\n",
        "    pe = np.zeros((context_size, emb_dim))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    self.positional_encoding = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.cast(x, tf.int32)\n",
        "    x = tf.one_hot(x, self.vocab_size)\n",
        "    x = tf.matmul(x, self.M)\n",
        "    seq_len = tf.minimum(self.context_size, tf.shape(x)[1])\n",
        "    x = x[:, :seq_len, :] + self.positional_encoding[:, :seq_len, :]\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"vocab_size\": self.vocab_size, \"context_size\":self.context_size})\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape + (self.emb_dim,)\n",
        "\n",
        "class attentify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  def __init__(self, emb_dim, head_dim, context_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_dim = head_dim\n",
        "    self.context_size = context_size\n",
        "    self.Q = self.add_weight(shape=(emb_dim, head_dim), initializer='glorot_uniform', name='Q', trainable=True)\n",
        "    self.K = self.add_weight(shape=(emb_dim, head_dim), initializer='glorot_uniform', name='K', trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    Qx = tf.matmul(x, self.Q)\n",
        "    Kx = tf.matmul(x, self.K)\n",
        "    A = tf.matmul(Qx, Kx, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "    mask = tf.linalg.band_part(tf.ones_like(A), -1, 0)\n",
        "    neg_inf = tf.fill(tf.shape(A), -1e-9)\n",
        "    A = tf.where(mask == 1, A, neg_inf)\n",
        "    A = tf.nn.softmax(A)\n",
        "    x = tf.matmul(A, x) + x\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"head_dim\": self.head_dim, \"context_size\":self.context_size})\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape\n",
        "\n",
        "class MLPify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  def __init__(self, emb_dim, expansion_multiplier, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.expansion_multiplier = expansion_multiplier\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.denseUp(x)\n",
        "    x = self.denseDown(x)\n",
        "    x = self.dropout(x,training=training)\n",
        "    return x\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.denseUp = Dense(self.emb_dim*self.expansion_multiplier, activation=\"gelu\")\n",
        "    self.denseDown = Dense(self.emb_dim, activation=\"gelu\")\n",
        "    self.dropout = Dropout(0.1)\n",
        "\n",
        "    self.denseUp.build(input_shape)  # input: (batch, context, emb_dim)\n",
        "    up_out_shape = self.denseUp.compute_output_shape(input_shape)\n",
        "    self.denseDown.build(up_out_shape)\n",
        "    self.dropout.build(up_out_shape)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"expansion_multiplier\": self.expansion_multiplier})\n",
        "    return config\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"expansion_multiplier\": self.expansion_multiplier})\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jbF9Qam2Giga"
      },
      "outputs": [],
      "source": [
        "model = load_model(\n",
        "    \"/content/drive/MyDrive/shakespeareGPT.keras\",\n",
        "    custom_objects={\"Custom>embedify\": embedify,\n",
        "                    \"Custom>attentify\": attentify,\n",
        "                    \"Custom>MLPify\": MLPify})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ron7jRDpG5jq"
      },
      "outputs": [],
      "source": [
        "def generate_next_word(text, gpt, top_word_range=2):\n",
        "  input_tokens = token2vec.encode(text)[-context_size:]\n",
        "  if len(input_tokens) < context_size:\n",
        "      input_tokens = [0] * (context_size - len(input_tokens)) + input_tokens\n",
        "\n",
        "  input_tensor = tf.convert_to_tensor([input_tokens], dtype=tf.int32)\n",
        "  pdf = gpt.predict(input_tensor, verbose=0)\n",
        "  heap = [(-pdf[0][i], i) for i in range(len(pdf[0]))]\n",
        "  heapq.heapify(heap)\n",
        "  pops = random.randint(0,top_word_range)\n",
        "  for _ in range(pops):\n",
        "    heapq.heappop(heap)\n",
        "  next_token_id = heapq.heappop(heap)[1]\n",
        "  next_word = token2vec.decode([next_token_id])\n",
        "  return next_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkNkIHW9Nmin",
        "outputId": "cb7f83a4-21b6-40a3-afc6-6d69d20fe696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upon this fair  I\n",
            " to my brother's the King. I have a woman'stis'd,\n",
            "     And all the world of my lord,\n",
            "     But I have the King of his Grace.\n",
            "     I'll be not, I am not to me, I have not, I amazement\n",
            "     And so far in my heart's a good.\n",
            "  Lear. And I am glad of the matter?\n",
            "  I'll have been a man, sir, sir.\n",
            "    And let me to the world's a man.\n",
            "     But let me, sir; I have a man, I'll make me, and I'll be a\n",
            "  CLOWALO\n",
            "     To make him to the King's a woman.\n",
            "    I am not, and so\n"
          ]
        }
      ],
      "source": [
        "text = \"Upon this fair \" #prompt\n",
        "for _ in range(150):\n",
        "  text += generate_next_word(text, model)\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}