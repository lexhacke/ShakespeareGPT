{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsSfO-8eS0UP",
        "outputId": "d6bd0b1a-f5c6-4e34-cd51-9c3575ca11a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1FclXPmTIaE",
        "outputId": "40433eb9-4389-47f7-d1db-929d12ef3082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "from tensorflow.keras  import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, LayerNormalization, Dense, Add, Concatenate, Dropout\n",
        "from keras.saving import register_keras_serializable\n",
        "from keras import config\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "token2vec = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XMRNWO0OTkc8"
      },
      "outputs": [],
      "source": [
        "#path = \"/content/drive/MyDrive/shakespeare.txt\"\n",
        "def load_shakespeare():\n",
        "  tokens = None\n",
        "  with open(path, \"r\") as file:\n",
        "    corpus = \"\"\n",
        "    for i,line in enumerate(file):\n",
        "        corpus+=line\n",
        "    file.close()\n",
        "    return token2vec.encode(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ECqdKrxxN7Ls"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable()\n",
        "class embedify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  # The build() function is used to create weights that depend on the input shape, we M's dimensions set in constructor so it's all good\n",
        "  # The get_config() function is used to return a dict of what input params are needed to load this model from a save\n",
        "  def __init__(self, emb_dim, vocab_size, context_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_size = context_size\n",
        "    self.M = self.add_weight(shape=(vocab_size, emb_dim), initializer='glorot_uniform', name='M', trainable=True)\n",
        "    position = np.arange(context_size)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, emb_dim, 2) * -(np.log(10000.0) / emb_dim))\n",
        "    pe = np.zeros((context_size, emb_dim))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    self.positional_encoding = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = tf.cast(x, tf.int32)\n",
        "    x = tf.one_hot(x, self.vocab_size)\n",
        "    x = tf.matmul(x, self.M)\n",
        "    seq_len = tf.minimum(self.context_size, tf.shape(x)[1])\n",
        "    x = x[:, :seq_len, :] + self.positional_encoding[:, :seq_len, :]\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"vocab_size\": self.vocab_size, \"context_size\":self.context_size})\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape + (self.emb_dim,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L_bmWXkzbkf3"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable()\n",
        "class attentify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  def __init__(self, emb_dim, head_dim, context_size, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.head_dim = head_dim\n",
        "    self.context_size = context_size\n",
        "    self.Q = self.add_weight(shape=(emb_dim, head_dim), initializer='glorot_uniform', name='Q', trainable=True)\n",
        "    self.K = self.add_weight(shape=(emb_dim, head_dim), initializer='glorot_uniform', name='K', trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    Qx = tf.matmul(x, self.Q)\n",
        "    Kx = tf.matmul(x, self.K)\n",
        "    A = tf.matmul(Qx, Kx, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "    mask = tf.linalg.band_part(tf.ones_like(A), -1, 0)\n",
        "    neg_inf = tf.fill(tf.shape(A), -1e-9)\n",
        "    A = tf.where(mask == 1, A, neg_inf)\n",
        "    A = tf.nn.softmax(A)\n",
        "    x = tf.matmul(A, x) + x\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"head_dim\": self.head_dim, \"context_size\":self.context_size})\n",
        "    return config\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_oBdSHshZbx"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable()\n",
        "class MLPify(tf.keras.layers.Layer):\n",
        "  # __NOTES__\n",
        "  def __init__(self, emb_dim, expansion_multiplier, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.emb_dim = emb_dim\n",
        "    self.expansion_multiplier = expansion_multiplier\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.denseUp(x)\n",
        "    x = self.denseDown(x)\n",
        "    x = self.dropout(x,training=training)\n",
        "    return x\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.denseUp = Dense(self.emb_dim*self.expansion_multiplier, activation=\"gelu\")\n",
        "    self.denseDown = Dense(self.emb_dim, activation=\"gelu\")\n",
        "    self.dropout = Dropout(0.1)\n",
        "\n",
        "    self.denseUp.build(input_shape)  # input: (batch, context, emb_dim)\n",
        "    up_out_shape = self.denseUp.compute_output_shape(input_shape)\n",
        "    self.denseDown.build(up_out_shape)\n",
        "    self.dropout.build(up_out_shape)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return input_shape\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\"emb_dim\": self.emb_dim, \"expansion_multiplier\": self.expansion_multiplier})\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Exb8hysYkfMB"
      },
      "outputs": [],
      "source": [
        "class GPTDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, tokens, context_size=100, vocab_size=100266, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.tokens = tokens\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_size = context_size\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(tokens) - context_size - 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "        X_batch = []\n",
        "        Y_batch = []\n",
        "\n",
        "        for start_idx in batch_indices:\n",
        "            x_tokens = self.tokens[start_idx:start_idx + self.context_size]\n",
        "            y_token = np.zeros(self.vocab_size, dtype=np.int32)\n",
        "            y_token[self.tokens[start_idx + self.context_size]] = 1\n",
        "\n",
        "            X_batch.append(x_tokens)\n",
        "            Y_batch.append(y_token)\n",
        "\n",
        "        X = np.array(X_batch, dtype=np.int32)  # shape: (batch_size, context_size)\n",
        "        Y = np.array(Y_batch, dtype=np.int32)  # shape: (batch_size,)\n",
        "        return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YbNAJBWGXdrd"
      },
      "outputs": [],
      "source": [
        "def create_model(emb_dim=300, vocab_size=100266, context_size=10, expansion_multiplier=4):\n",
        "  inputs = Input(shape=(context_size,))\n",
        "  x = embedify(emb_dim, vocab_size, context_size)(inputs)\n",
        "  a1 = attentify(emb_dim, emb_dim//6, context_size)(x)\n",
        "  a2 = attentify(emb_dim, emb_dim//6, context_size)(x)\n",
        "  a3 = attentify(emb_dim, emb_dim//6, context_size)(x)\n",
        "  x = Concatenate(axis=-1)([a1, a2, a3])\n",
        "  x = LayerNormalization()(x)\n",
        "  x = Dense(300)(x)\n",
        "  x = MLPify(emb_dim, expansion_multiplier)(x)\n",
        "  x = LayerNormalization()(x)\n",
        "  x = x[:, -1, :]\n",
        "  x = Dense(vocab_size)(x)\n",
        "  return Model(inputs, x, name=\"lexGPT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhLq0-XPxhpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7565fb1-d372-4f43-dc24-b0289c10d30c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 100266), dtype=float32, numpy=\n",
              "array([[-0.09560445,  0.06490345,  0.05151351, ..., -0.00963269,\n",
              "         0.05324729, -0.00521233]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "gpt = create_model()\n",
        "gpt.compile(optimizer=\"adam\", loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
        "#Force layer building by calling model once\n",
        "dummy_data = np.random.randint(0, 100266, size=(1, 10))\n",
        "gpt(dummy_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0TDMRRokhma",
        "outputId": "fef58367-e13a-4b2a-e53e-1597817b0cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7990s\u001b[0m 174ms/step - loss: 6.2314\n",
            "Epoch 2/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7946s\u001b[0m 173ms/step - loss: 5.4144\n",
            "Epoch 3/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8005s\u001b[0m 173ms/step - loss: 4.9992\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d3a8a00af10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "gpt.fit(GPTDataGen(load_shakespeare()), epochs=3,verbose=1, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MFslqZ9kiAs"
      },
      "outputs": [],
      "source": [
        "gpt.save(\"/content/drive/MyDrive/shakespeareGPT-partial-train.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l7IhlGGGYAbr"
      },
      "outputs": [],
      "source": [
        "gpt = load_model(\"/content/drive/MyDrive/shakespeareGPT-partial-train.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.fit(GPTDataGen(load_shakespeare()), epochs=3,verbose=1, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj9yUdvwVB3s",
        "outputId": "6c31f41e-bd67-44af-c563-163ba231c2ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1519s\u001b[0m 33ms/step - loss: 4.7965\n",
            "Epoch 2/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1504s\u001b[0m 33ms/step - loss: 4.7092\n",
            "Epoch 3/3\n",
            "\u001b[1m45884/45884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1504s\u001b[0m 33ms/step - loss: 4.6420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e9874917b90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.save(\"/content/drive/MyDrive/shakespeareGPT.keras\")"
      ],
      "metadata": {
        "id": "IR9yXEUxVCxw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_4HFswxq_C8"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}